{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "4e5342c6-1db3-496e-a4c8-b1366bbd271a",
      "cell_type": "markdown",
      "source": "**Decision Trees with Sample/Feature Subsetting**\n- max_samples: subset of samples per tree\n- max_features: subset of features per split\n- min_samples_split: minimum samples for split\n- min_samples_leaf: minimum samples in leaf",
      "metadata": {}
    },
    {
      "id": "3c20f707-133a-458e-befd-6839eb9d07c7",
      "cell_type": "code",
      "source": "from sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\ntree = DecisionTreeClassifier(\n    max_features=\"sqrt\",\n    min_samples_split=5,\n    min_samples_leaf=2\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "846cc40a-b8ee-4c3b-ae1d-e3b2cc5c3ef3",
      "cell_type": "markdown",
      "source": "**Random Forest**\n- Ensemble of decision trees\n- Bootstrap sampling (bagging)\n- Random feature selection\n- Parallel training\n- Reduces overfitting",
      "metadata": {}
    },
    {
      "id": "1aba57e5-34c0-4667-bc18-23c37e2acaf9",
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nclass RandomForestExample:\n    def __init__(self, n_estimators=100):\n        self.rf = RandomForestClassifier(\n            n_estimators=n_estimators,\n            max_features=\"sqrt\",\n            bootstrap=True,\n            oob_score=True\n        )\n\n    def train_and_evaluate(self, X, y):\n        # Split data\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2\n                                                           )\n        # Train\n        self.rf.fit(X_train, y_train)\n        # Evaluate\n        print(f\"OOB score: {self.rf.oob_score_:.3f}\")\n        print(f\"Test score: {self.rf.score(X_test, y_test):.3f}\")\n\n        return self.rf.feature_importances_",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "780772ec-bf65-4ba3-b7bc-4e3992d0a4c5",
      "cell_type": "markdown",
      "source": "**Gradient Boosting**\n- Sequential training\n- Each tree correct previous errors\n- Learning rate controls contribution\n- More prone to overfitting than RF",
      "metadata": {}
    },
    {
      "id": "fc6aeef6-6d97-46ed-9f29-0576db94103e",
      "cell_type": "code",
      "source": "from sklearn.ensemble import GradientBoostingClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\n\nclass GradientBoostingExample:\n    def __init__(self):\n        self.gbm = GradientBoostingClassifier(\n            n_estimators=100,\n            learning_rate=0.1,\n            max_depth=3,\n            subsample=0.8\n        )\n\n        # XGBoost\n        self.xgb = xgb.XGBClassifier(\n            n_estimators=100,\n            learning_rate=0.1,\n            max_depth=3,\n            subsample=0.8,\n            colsample_bytree=0.8\n        )\n\n        # LightGBM\n        self.lgb = lgb.LGBMClassifier(\n            n_estimators=100,\n            learning_rate=0.1,\n            max_depth=3,\n            subsample=0.8\n        )\n\n    def compare_boosting(self, X, y):\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2\n        )\n\n        result = {}\n        for name, model in [\n            (\"GBM\", self.gbm),\n            (\"XGBoost\", self.xgb),\n            (\"LightGBM\", self.lgb)\n        ]:\n            model.fit(X_train, y_train)\n            score = model.score(X_test, y_test)\n            results[name] = score\n\n        return results",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9064d2e2-9cc8-428e-8e55-f43d6184a89f",
      "cell_type": "markdown",
      "source": "**Feature Importance Measures**\n- Gini/Entropy importance (default)\n- Permutation importance\n- SHAP values",
      "metadata": {}
    },
    {
      "id": "57885c07-fadb-48bd-b345-f7db3ac4f004",
      "cell_type": "code",
      "source": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score\n\nclass TreeFeatureImportance:\n    def __init__(self, model, feature_names):\n        self.model = model\n        self.feature_names = feature_names\n\n    def get_importance_types(self):\n        from sklearn.inspection import permutation_importance\n        import shap\n\n        importances = {\n            \"gini\": self.model.feature_importances_,\n            \"permutation\": permutation_importance(\n                self.model, X_test, y_test\n            ).importances_mean\n        }\n        # SHAP values\n        explainer = shap.TreeExplainer(self.model)\n        shap_values = explainer.shap_values(X_test)\n        importances[\"shap\"] = np.abs(shap_values).mean(axis=0)\n\n        return importances\n\n    def plot_importance(self, importance_type=\"gini\"):\n        importance = self.get_importance_types()[importance_type]\n        imp_df = pd.DataFrame({\n            \"feature\": self.feature_names,\n            \"importance\": importance\n        }).sort_values(\"importance\", ascending=False)\n        plt.figure(figsize=(10, 6))\n        plt.bar(imp_df[\"feature\"], imp_df[\"importance\"])\n        plt.xticks(rotation=45)\n        plt.title(f\"Feature Importance ({importance_type})\")\n        plt.tight_layout()\n        plt.show\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef compare_tree_models(X, y):\n    # Initialize models with proper parameters\n    models = {\n        'Random Forest': RandomForestClassifier(random_state=42),\n        'GradientBoosting': GradientBoostingClassifier(random_state=42),\n        'XGBoost': xgb.XGBClassifier(\n            use_label_encoder=False,\n            eval_metric='logloss',\n            random_state=42\n        ),\n        'LightGBM': lgb.LGBMClassifier(random_state=42)\n    }\n    \n    results = {}\n    for name, model in models.items():\n        try:\n            # Cross validation\n            scores = cross_val_score(model, X, y, cv=5)\n            results[name] = {\n                'mean_score': scores.mean(),\n                'std_score': scores.std(),\n                'scores': scores\n            }\n            print(f\"{name}:\")\n            print(f\"Mean CV Score: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n        except Exception as e:\n            print(f\"Error with {name}: {str(e)}\")\n            continue\n    \n    # Visualization of results\n    plt.figure(figsize=(10, 6))\n    plt.boxplot([results[model]['scores'] for model in results.keys()])\n    plt.xticks(range(1, len(results) + 1), results.keys(), rotation=45)\n    plt.title('Model Comparison')\n    plt.ylabel('Score')\n    plt.tight_layout()\n    plt.show()\n    \n    return results",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9f25fbd9-c13c-4c6d-a7eb-d2126d278240",
      "cell_type": "code",
      "source": "# Example\nX = np.random.randn(1000, 20)\ny = np.random.randint(0, 2, 1000)\nresults = compare_tree_models(X, y)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}